# -*- coding: utf-8 -*-
"""Record 1: 1833 FeatureSelection + Relu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bgq_C7zzYKxomHYfBjbVfDqAM6_jQ0J_

Common Structures
"""

#Load 
import time
t0 = time.time()

import numpy as np
import random
import pandas as pd
import matplotlib.pyplot as plt
import os
import copy
import seaborn as sns

from sklearn import preprocessing
from sklearn.metrics import log_loss
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import QuantileTransformer

import sys
sys.path.append('/content/drive/My Drive/Colab Notebooks')
from ml_stratifiers import MultilabelStratifiedKFold

from sklearn.feature_selection import VarianceThreshold

def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    
seed_everything(seed=42)

def loading_files():

  train_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MoA Prediction/train_features.csv')
  train_targets_scored = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MoA Prediction/train_targets_scored.csv')
  train_targets_nonscored = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MoA Prediction/train_targets_nonscored.csv')

  test_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MoA Prediction/test_features.csv')
  sample_submission = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MoA Prediction/sample_submission.csv')

  return train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission

def processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission):
  
  GENES = [col for col in train_features.columns if col.startswith('g-')]
  #CELLS = [col for col in train_features.columns if col.startswith('c-')]

  for col in (GENES):

    transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution="normal")
    vec_len = len(train_features[col].values)
    vec_len_test = len(test_features[col].values)
    raw_vec = train_features[col].values.reshape(vec_len, 1)
    transformer.fit(raw_vec)

    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]
    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]
  
  return train_features,test_features

def VarianceFilter(num,train_features,test_features):
  var_thresh = VarianceThreshold(threshold=0.5)
  data = train_features.append(test_features)
  data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])

  train_features_transformed = data_transformed[ : train_features.shape[0]]
  test_features_transformed = data_transformed[-test_features.shape[0] : ]


  train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\
                                columns=['sig_id','cp_type','cp_time','cp_dose'])

  train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)


  test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\
                              columns=['sig_id','cp_type','cp_time','cp_dose'])

  test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)

  return train_features,test_features

def data_cleaning(train_features,test_features):

  train = train_features.merge(train_targets_scored, on='sig_id')
  train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)
  test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)

  target = train[train_targets_scored.columns]
  train = train.drop('cp_type', axis=1)
  test = test.drop('cp_type', axis=1)

  return train,test,target

def createfolds(train,number):

  folds = train.copy()

  mskf = MultilabelStratifiedKFold(n_splits=number)

  for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):
      folds.loc[v_idx, 'kfold'] = int(f)

  folds['kfold'] = folds['kfold'].astype(int)

  return folds

class MoADataset:
    def __init__(self, features, targets):
        self.features = features
        self.targets = targets
        
    def __len__(self):
        return (self.features.shape[0])
    
    def __getitem__(self, idx):
        dct = {
            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),
            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            
        }
        return dct
    
class TestDataset:
    def __init__(self, features):
        self.features = features
        
    def __len__(self):
        return (self.features.shape[0])
    
    def __getitem__(self, idx):
        dct = {
            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)
        }
        return dct

def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):
    model.train()
    final_loss = 0
    
    for data in dataloader:
        optimizer.zero_grad()
        inputs, targets = data['x'].to(device), data['y'].to(device)
#         print(inputs.shape)
        outputs = model(inputs)
        loss = loss_fn(outputs, targets)
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        final_loss += loss.item()
        
    final_loss /= len(dataloader)
    
    return final_loss


def valid_fn(model, loss_fn, dataloader, device):
    model.eval()
    final_loss = 0
    valid_preds = []
    
    for data in dataloader:
        inputs, targets = data['x'].to(device), data['y'].to(device)
        outputs = model(inputs)
        loss = loss_fn(outputs, targets)
        
        final_loss += loss.item()
        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())
        
    final_loss /= len(dataloader)
    valid_preds = np.concatenate(valid_preds)
    
    return final_loss, valid_preds

def inference_fn(model, dataloader, device):
    model.eval()
    preds = []
    
    for data in dataloader:
        inputs = data['x'].to(device)

        with torch.no_grad():
            outputs = model(inputs)
        
        preds.append(outputs.sigmoid().detach().cpu().numpy())
        
    preds = np.concatenate(preds)
    
    return preds

def process_data(data):
    
    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])
    
    return data

def run_training(fold, seed):
    
    seed_everything(seed)
    
    train = process_data(folds)
    test_ = process_data(test)
    
    trn_idx = train[train['kfold'] != fold].index
    val_idx = train[train['kfold'] == fold].index
    
    train_df = train[train['kfold'] != fold].reset_index(drop=True)
    valid_df = train[train['kfold'] == fold].reset_index(drop=True)
    
    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values
    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values
    
    train_dataset = MoADataset(x_train, y_train)
    valid_dataset = MoADataset(x_valid, y_valid)
    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    model = Model(
        num_features=num_features,
        num_targets=num_targets,
        hidden_size=hidden_size,
    )
    
    model.to(DEVICE)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, 
                                              max_lr=5e-3, epochs=EPOCHS, steps_per_epoch=len(trainloader))
    
    loss_fn = nn.BCEWithLogitsLoss()
    
    early_stopping_steps = EARLY_STOPPING_STEPS
    early_step = 0
    
    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))
    best_loss = np.inf
    
    for epoch in range(EPOCHS):
        
        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)
        print(f"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}")
        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)
        print(f"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}")
        
        if valid_loss < best_loss:
            
            best_loss = valid_loss
            oof[val_idx] = valid_preds
            torch.save(model.state_dict(), f"FOLD{fold}_.pth")
        
        elif(EARLY_STOP == True):
            
            early_step += 1
            if (early_step >= early_stopping_steps):
                break
            
    
    #--------------------- PREDICTION---------------------
    x_test = test_[feature_cols].values
    testdataset = TestDataset(x_test)
    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)
    
    model = Model(
        num_features=num_features,
        num_targets=num_targets,
        hidden_size=hidden_size,
    )
    
    model.load_state_dict(torch.load(f"FOLD{fold}_.pth"))
    model.to(DEVICE)
    
    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))
    predictions = inference_fn(model, testloader, DEVICE)
    
    return oof, predictions

def run_k_fold(NFOLDS, seed):
    oof = np.zeros((len(train), len(target_cols)))
    predictions = np.zeros((len(test), len(target_cols)))
    
    for fold in range(NFOLDS):
        oof_, pred_ = run_training(fold, seed)
        
        predictions += pred_ / NFOLDS
        oof += oof_
        
    return oof, predictions

def decision_tree_selection():
  train_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MoA Prediction/train_features.csv')
  train_targets_scored = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MoA Prediction/train_targets_scored.csv')
  
  train = train_features.merge(train_targets_scored, on='sig_id')
  train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)
  target = train[train_targets_scored.columns].iloc[:,1:]

  from sklearn.model_selection import train_test_split

  target = train[train_targets_scored.columns].iloc[:,1:]
  traindata = train.iloc[:,4:876]

  X_train, X_test, y_train, y_test = train_test_split(traindata, target, test_size=0.3, random_state=42)

  from sklearn.model_selection import cross_val_score
  from sklearn.tree import DecisionTreeClassifier

  clf = DecisionTreeClassifier(random_state=0)
  empty = []

  for i in range(X_test.shape[1]-2):
    a = X_test.iloc[:,i:i+2]
    score = sum(cross_val_score(clf, a, y_test, cv=2))/2
    empty.append([list(a.columns),score])
    print([list(a.columns),score])

  empty1 = pd.DataFrame(empty)

  empty1.columns = ['ParameterNames', 'CV_Values']
  empty1 = empty1.sort_values('CV_Values',ascending= False)
  threhold = empty1.iloc[:,1][round(len(empty1.iloc[:,1])*0.2)]
  filtered = empty1[empty1['CV_Values'] > threhold]

  predictor_list = []
  for element in filtered['ParameterNames']:
    predictor_list += element

  start_predicator = list(set(predictor_list))

  return start_predicator

"""First Model"""

train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission = loading_files()
train_features,test_features = processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission)

# GENES

GENES = [col for col in train_features.columns if col.startswith('g-')]
CELLS = [col for col in train_features.columns if col.startswith('c-')]

n_comp = 200

data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])
data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))
train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]

train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])
test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])

# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]
train_features = pd.concat((train_features, train2), axis=1)
test_features = pd.concat((test_features, test2), axis=1)

#CELLS
n_comp = 20

data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])
data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))
train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]

train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])
test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])

# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]
train_features = pd.concat((train_features, train2), axis=1)
test_features = pd.concat((test_features, test2), axis=1)

train_features,test_features = VarianceFilter(0.5,train_features,test_features)
train,test,target = data_cleaning(train_features,test_features)
target_cols = target.drop('sig_id', axis=1).columns.values.tolist()
folds = createfolds(train,15)

class Model(nn.Module):
    
    def __init__(self, num_features, num_targets, hidden_size):
        super(Model, self).__init__()
        self.batch_norm1 = nn.BatchNorm1d(num_features)
        self.dropout1 = nn.Dropout(0.11)
        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))
        
        self.batch_norm2 = nn.BatchNorm1d(hidden_size)
        self.dropout2 = nn.Dropout(0.11)
        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))

        #self.batch_norm3 = nn.BatchNorm1d(hidden_size)
        #self.dropout3 = nn.Dropout(0.11)
        #self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))
        
        self.batch_norm4 = nn.BatchNorm1d(hidden_size)
        self.dropout4 = nn.Dropout(0.11)
        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))
    
    def forward(self, x):
        x = self.batch_norm1(x)
        x = self.dropout1(x)
        x = F.relu(self.dense1(x))
        
        x = self.batch_norm2(x)
        x = self.dropout2(x)
        x = F.relu(self.dense2(x))

        #x = self.batch_norm3(x)
        #x = self.dropout3(x)
        #x = F.relu(self.dense3(x))
        
        x = self.batch_norm4(x)
        x = self.dropout4(x)
        x = self.dense4(x)
        
        return x

feature_cols = [c for c in process_data(folds).columns if c not in target_cols]
feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]

# HyperParameters

DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')
EPOCHS = 35
BATCH_SIZE = 128
LEARNING_RATE = 0.002
WEIGHT_DECAY = 1e-5
NFOLDS = 15
EARLY_STOPPING_STEPS = 10
EARLY_STOP = False

num_features=len(feature_cols)
num_targets=len(target_cols)
hidden_size=1024

# Averaging on multiple SEEDS

SEED = [0, 1,2,3,4]#,9,10,11,12,13,14,15]
oof = np.zeros((len(train), len(target_cols)))
predictions = np.zeros((len(test), len(target_cols)))

for seed in SEED:
    print("Seed:",seed)
    oof_, predictions_ = run_k_fold(NFOLDS, seed)
    oof += oof_ / len(SEED)
    predictions += predictions_ / len(SEED)

train[target_cols] = oof
test[target_cols] = pd.DataFrame(predictions)
test3 = test

t1 = time.time()
total = t1-t0

print(total)

valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)


y_true = train_targets_scored[target_cols].values
y_pred = valid_results[target_cols].values

score = 0
for i in range(len(target_cols)):
    score_ = log_loss(y_true[:, i], y_pred[:, i])
    score += score_ / target.shape[1]
    
print("CV log_loss: ", score)

t1 = time.time()
total = t1-t0

print(total)

"""Second Model"""

train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission = loading_files()
train_features,test_features = processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission)

# GENES
GENES = [col for col in train_features.columns if col.startswith('g-')]
CELLS = [col for col in train_features.columns if col.startswith('c-')]

n_comp = 200

data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])
a = pd.DataFrame(data.std()/data.mean())
b = a[a[0]>1]
data = data[b.index]
data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data))
data2 = np.append(np.power(data2[0:20],3), data2[10:],axis = 0)
train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]

train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])
test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])

# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]
train_features = pd.concat((train_features, train2), axis=1)
test_features = pd.concat((test_features, test2), axis=1)

#CELLS
n_comp = 15

data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])
data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data))
data2 = np.append(np.power(data2[0:20],3), data2[10:],axis = 0)
train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]

train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])
test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])

# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]
train_features = pd.concat((train_features, train2), axis=1)
test_features = pd.concat((test_features, test2), axis=1)

train_features,test_features = VarianceFilter(0.5,train_features,test_features)
train,test1,target = data_cleaning(train_features,test_features)
target_cols = target.drop('sig_id', axis=1).columns.values.tolist()
folds = createfolds(train,5)

class Model(nn.Module):
    
    def __init__(self, num_features, num_targets, hidden_size):
        super(Model, self).__init__()
        self.batch_norm1 = nn.BatchNorm1d(num_features)
        self.dropout1 = nn.Dropout(0.2)
        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))
        
        self.batch_norm2 = nn.BatchNorm1d(hidden_size)
        self.dropout2 = nn.Dropout(0.5)
        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))
        
        self.batch_norm4 = nn.BatchNorm1d(hidden_size)
        self.dropout4 = nn.Dropout(0.5)
        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))
    
    def forward(self, x):
        x = self.batch_norm1(x)
        x = self.dropout1(x)
        x = F.relu(self.dense1(x))
        
        x = self.batch_norm2(x)
        x = self.dropout2(x)
        x = F.relu(self.dense2(x))
 
        x = self.batch_norm4(x)
        x = self.dropout4(x)
        x = self.dense4(x)
        
        return x

feature_cols = [c for c in process_data(folds).columns if c not in target_cols]
feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]

# HyperParameters

DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')
EPOCHS = 30
BATCH_SIZE = 128
LEARNING_RATE = 2.15e-05
WEIGHT_DECAY = 1e-5
NFOLDS = 5
EARLY_STOPPING_STEPS = 10
EARLY_STOP = False

num_features=len(feature_cols)
num_targets=len(target_cols)
hidden_size=1024

# Averaging on multiple SEEDS

SEED = [0,1,2,3]#,4,5,6,7]
oof = np.zeros((len(train), len(target_cols)))
predictions = np.zeros((len(test), len(target_cols)))

for seed in SEED:
    print("Seed:",seed)
    oof_, predictions_ = run_k_fold(NFOLDS, seed)
    oof += oof_ / len(SEED)
    predictions += predictions_ / len(SEED)

train[target_cols] = oof
test1[target_cols] = pd.DataFrame(predictions)

valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)


y_true = train_targets_scored[target_cols].values
y_pred1 = valid_results[target_cols].values

score = 0
for i in range(len(target_cols)):
    score_ = log_loss(y_true[:, i], y_pred1[:, i])
    score += score_ / target.shape[1]
    
print("CV log_loss: ", score)

t1 = time.time()
total = t1-t0

print(total)

"""Third TabNet Model"""

!pip install pytorch-tabnet

import sys
sys.path.append('/content/drive/My Drive/Colab Notebooks/')
from ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit

import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
import torch.optim as optim
import torch.nn.functional as F
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.model_selection import StratifiedKFold
from pytorch_tabnet.tab_model import TabNetRegressor
import numpy as np
import pandas as pd 
from sklearn.metrics import roc_auc_score

import os
import random
import sys
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
from tqdm import tqdm
from sklearn.metrics import log_loss

def seed_everything(seed_value):
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    os.environ['PYTHONHASHSEED'] = str(seed_value)
    
    if torch.cuda.is_available(): 
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
seed_everything(42)

#Data Loading

data_path = "/content/drive/My Drive/Colab Notebooks/MoA Prediction/"
train = pd.read_csv(data_path+'train_features.csv')

start_predicator = decision_tree_selection()
final_predicator = ['sig_id','cp_type','cp_time','cp_dose'] + start_predicator

train = train[final_predicator]
train.drop(columns=["sig_id"], inplace=True)

train_targets_scored = pd.read_csv(data_path+'train_targets_scored.csv')
train_targets_scored.drop(columns=["sig_id"], inplace=True)

test = pd.read_csv(data_path+'test_features.csv')
test = test[final_predicator]
test.drop(columns=["sig_id"], inplace=True)

GENES = [col for col in train.columns if col.startswith('g-')]

for col in (GENES):

  transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution="normal")
  vec_len = len(train[col].values)
  vec_len_test = len(test[col].values)
  raw_vec = train[col].values.reshape(vec_len, 1)
  transformer.fit(raw_vec)

  train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]
  test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]

submission = pd.read_csv(data_path+'sample_submission.csv')

remove_vehicle = False

if remove_vehicle:
    kept_index = train['cp_type']=='trt_cp'
    train = train.loc[kept_index].reset_index(drop=True)
    train_targets_scored = train_targets_scored.loc[kept_index].reset_index(drop=True)

train["cp_type"] = (train["cp_type"]=="trt_cp") + 0
train["cp_dose"] = (train["cp_dose"]=="D1") + 0

test["cp_type"] = (test["cp_type"]=="trt_cp") + 0
test["cp_dose"] = (test["cp_dose"]=="D1") + 0

X_test = test.values

MAX_EPOCH=200
tabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.3,
                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,
                     optimizer_params=dict(lr=1e-2, weight_decay=1e-5),
                     mask_type='entmax',
                     scheduler_params=dict(mode="min",
                                           patience=5,
                                           min_lr=1e-5,
                                           factor=0.9,),
                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,
                     verbose=10,
                     )

from sklearn.metrics import log_loss
from pytorch_tabnet.metrics import Metric
from sklearn.metrics import roc_auc_score, log_loss

class LogitsLogLoss(Metric):
    """
    LogLoss with sigmoid applied
    """

    def __init__(self):
        self._name = "logits_ll"
        self._maximize = False

    def __call__(self, y_true, y_pred):
        """
        Compute LogLoss of predictions.

        Parameters
        ----------
        y_true: np.ndarray
            Target matrix or vector
        y_score: np.ndarray
            Score matrix or vector

        Returns
        -------
            float
            LogLoss of predictions vs targets.
        """
        logits = 1 / (1 + np.exp(-y_pred))
        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)
        
        return np.mean(-aux)

scores_auc_all= []
test_cv_preds = []

NB_SPLITS = 10
mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)
oof_preds = []
oof_targets = []
scores = []
scores_auc = []

for seed in [0,1]:
  print('Seed',seed)

  for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, train_targets_scored)):
      print("FOLDS : ", fold_nb)

      ## model
      X_train, y_train = train.values[train_idx, :], train_targets_scored.values[train_idx, :]
      X_val, y_val = train.values[val_idx, :], train_targets_scored.values[val_idx, :]
      model = TabNetRegressor(**tabnet_params)

      model.fit(X_train=X_train,
                y_train=y_train,
                eval_set=[(X_val, y_val)],
                eval_name = ["val"],
                eval_metric = ["logits_ll"],
                max_epochs=MAX_EPOCH,
                patience=20, batch_size=1024, virtual_batch_size=128,
                num_workers=1, drop_last=False,
                # use binary cross entropy as this is not a regression problem
                loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)

      preds_val = model.predict(X_val)
      # Apply sigmoid to the predictions
      preds =  1 / (1 + np.exp(-preds_val))
      score = np.min(model.history["val_logits_ll"])
  #     name = cfg.save_name + f"_fold{fold_nb}"
  #     model.save_model(name)
      ## save oof to compute the CV later
      oof_preds.append(preds_val)
      oof_targets.append(y_val)
      scores.append(score)

      # preds on test
      preds_test = model.predict(X_test)
      test_cv_preds.append(1 / (1 + np.exp(-preds_test)))

oof_preds_all = np.concatenate(oof_preds)
oof_targets_all = np.concatenate(oof_targets)
test_preds_all = np.stack(test_cv_preds)

aucs = [] #roc_auc_score
for task_id in range(oof_preds_all.shape[1]):
    y_true=oof_targets_all[:, task_id]
    y_score=oof_preds_all[:, task_id]
    aucs.append(roc_auc_score(y_true,y_score))
print(f"Overall AUC : {np.mean(aucs)}")
print(f"Average CV : {np.mean(scores)}")

all_feat = [col for col in submission.columns if col not in ["sig_id"]]
submission[all_feat] = test_preds_all.mean(axis=0)
# set control to 0
submission.loc[test['cp_type']==0, submission.columns[1:]] = 0
TabNetResult = submission

t1 = time.time()
print(t1-t0)

"""Keras Model"""

!pip install tensorflow_addons==0.10.0

import os
import random

import warnings
warnings.resetwarnings()
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()


import tensorflow as tf
from keras.initializers import Constant
from tensorflow.keras import callbacks, losses, backend
import tensorflow_addons as tfa

from sklearn.decomposition import PCA
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import KFold
from sklearn.preprocessing import OneHotEncoder, QuantileTransformer, StandardScaler
from statistics import mean, median

os.environ['PYTHONHASHSEED'] = '1068'
np.random.seed(1068)
tf.random.set_seed(1068)
random.seed(1068)

def _path(file):
    return "/content/drive/My Drive/Colab Notebooks/MoA Prediction/" + file

train = pd.read_csv(_path("train_features.csv"))
test = pd.read_csv(_path("test_features.csv"))

target = pd.read_csv(_path("train_targets_scored.csv"))
sample_sub = pd.read_csv(_path("sample_submission.csv"))

GENES = [col for col in train.columns if col.startswith('g-')]

for col in (GENES):
  transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution="normal")
  vec_len = len(train[col].values)
  vec_len_test = len(test[col].values)
  raw_vec = train[col].values.reshape(vec_len, 1)
  transformer.fit(raw_vec)

  train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]
  test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]

target_col = target.drop('sig_id', axis=1).columns
ctl_train = train['cp_type'] == 'ctl_vehicle'
ctl_test = test['cp_type'] == 'ctl_vehicle'
no_ctl_train = np.logical_not(ctl_train)
no_ctl_test = np.logical_not(ctl_test)
sample_sub.loc[:, target_col] = 0

def preprocess(train, test, target, params):
    
    
    train = train.drop('sig_id', axis=1)
    test = test.drop('sig_id', axis=1)
    target = target.drop('sig_id', axis=1)
    
    train_len = train.shape[0]
    test_len = test.shape[0]
    
    print('train shape:', train.shape)
    print('test shape:', test.shape)

    df = pd.concat([train, test], ignore_index=True)
    
    GENES = [col for col in df.columns if col.startswith('g-')]
    CELLS = [col for col in df.columns if col.startswith('c-')]
    num_cols = GENES + CELLS
    
    
    print('label encoding...')
    df['cp_type'] = df['cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})
    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})
    print('finish label encoding...')

    
    print('PCA:', params['pca'])
    if params['pca']:
        g_n_comp = params['pca']['g_n_comp']
        c_n_comp = params['pca']['c_n_comp']
        g_datas = df[GENES]
        g_datas_transformed = (PCA(n_components=g_n_comp, random_state=1068).fit_transform(g_datas[GENES]))
        g_datas_transformed = pd.DataFrame(g_datas_transformed, columns=[f'pca_G-{i}' for i in range(g_n_comp)])
        df = pd.concat((df, g_datas_transformed), axis=1)
        c_datas = df[CELLS]
        c_datas_transformed = (PCA(n_components=c_n_comp, random_state=1068).fit_transform(g_datas[GENES]))
        c_datas_transformed = pd.DataFrame(c_datas_transformed, columns=[f'pca_G-{i}' for i in range(c_n_comp)])
        df = pd.concat((df, c_datas_transformed), axis=1)
        print('finish PCA.....')
        print('train shape:', df[:train_len].shape)
        print('test shape:', df[train_len:].shape)
    
    
    print('numerical_encoding:', params['numerical_encoding'])
    if params['numerical_encoding'] == 'standart_scale':
        scaler = StandardScaler()
        df[num_cols] = scaler.fit_transform(df[num_cols])
        print('finish encoding...')
    elif params['numerical_encoding'] == 'quantiletransform':
        qt = QuantileTransformer(output_distribution='normal', random_state=1068)
        df[num_cols] = qt.fit_transform(df[num_cols])
        print('finish encoding...')
    
    

    print('VarianceThreshold:', params['variance_encoding'])
    if not params['variance_encoding']:
        train = df[:train_len].values
        test = df[train_len:].values
        target = target.values
    else:
        print('do variance encoding....')
        threshold = params['variance_encoding']
        var_thresh = VarianceThreshold(threshold=threshold)
        df = var_thresh.fit_transform(df)
        print('finish variance encoding')
        print('train shape:', df[:train_len].shape)
        print('test shape:', df[train_len:].shape)
        train = df[:train_len]
        test = df[train_len:]
        target = target.values
    
    
    print('###################')
    print('finish all proccess')
    print('train shape:', train.shape)
    print('test shape:', test.shape)
    print('target shape:', target.shape)
    print('###################')
    
    return train, test, target

# params = {'pca':{'g_n_comp':28, 'c_n_comp':5},
#           'numerical_encoding':'quantiletransform',
#           'categorycal_encoding':'label_encoding',
#           'variance_encoding':0.5
#         }


params = {'pca': False,
         'numerical_encoding':'quantiletransform',
         'categorycal_encoding':'label_encoding',
         'variance_encoding':False
}

train_x, test_x, train_y = preprocess(train, test, target, params)

p_min = 0.001
p_max = 0.999

def logloss(y_true, y_pred):
    y_pred = tf.clip_by_value(y_pred,p_min,p_max)
    return -backend.mean(y_true * backend.log(y_pred) + (1 - y_true) * backend.log(1 - y_pred))

class MLP:
    def __init__(self, input_shape, activation, output_bias):
        
        output_bias = tf.keras.initializers.Constant(output_bias)
        
        self.model = tf.keras.Sequential([
        
        tf.keras.layers.Input(input_shape),
        
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.25),
        tfa.layers.WeightNormalization(tf.keras.layers.Dense(512, activation=activation)),
        
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.60),
        tfa.layers.WeightNormalization(tf.keras.layers.Dense(256, activation=activation)),

        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.45),
        tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation='sigmoid', bias_initializer=output_bias))
        
        ])
        
        
        optimizer = tfa.optimizers.Lookahead(
            tfa.optimizers.AdamW(weight_decay=1e-5),
            sync_period=5
        )
        
        
        self.model.compile(optimizer=optimizer, 
                           loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0008),
                           metrics=logloss)
        
        
        
    def fit(self, train_x,train_y, val_x, val_y, epochs, batch_size, verbose):
        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=5e-6)
        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=5e-6, patience=5, mode='min',restore_best_weights=True)
        history = self.model.fit(train_x,
                                 train_y,
                                 epochs=epochs,
                                 validation_data=(val_x, val_y),
                                 batch_size = batch_size,
                                 verbose = verbose,
                                 callbacks = [reduce_lr, early_stopping]
                                )
        return history
    
    
    def predict(self,test_x):   
        pred = self.model.predict(test_x)
        return pred

def show_loss(losses):
    for loss in losses:
        print(' -> {:.6f}'.format(loss), end='')
    print()

def plot_loss(history, i, seed):
    title = 'loss fold {} in seed {}'.format(i, seed)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title(title)
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

def run(train_x, train_y, test_x, n_seed, n_fold, epoch, debug, verbose, output_bias, activation, plot):
    start_run = time.time()
    preds = sample_sub[:]
    val_losses = []
    if debug:
        print('run debug mode...')
        train_x = train_x[:10]
        train_y = train_y[:10]
        # test_x = test_x[:10]   
        epoch = 2
        n_fold = 2
        n_seed = 2
        plot = True
        verbose = 1
    for seed in range(1068, n_seed + 1068):
        print('###############')
        print('run_seed:', seed)
        print('###############')
        start = time.time()
        mskf = MultilabelStratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)         
        for (i, (train_idx, val_idx)) in enumerate(mskf.split(train_x, train_y), 1):
            model = MLP(train_x.shape[1], activation, output_bias)
            history = model.fit(train_x[train_idx],
                                train_y[train_idx], 
                                train_x[val_idx], 
                                train_y[val_idx],
                                epochs=epoch, 
                                batch_size=128,
                                verbose=verbose
                                )
            
            if plot:
                plot_loss(history, i, seed)
            
            print('finish run fold', i, 'with', len(history.history['loss']), 'epoch')
            print('train_loss of last 5 epoch:', end='')
            show_loss(history.history['loss'][-5:])
            print('val_loss of last 5 epoch  :', end='')
            show_loss(history.history['val_loss'][-5:])
            val_losses.append(history.history['val_loss'][-1])
            
            
            pred = model.predict(test_x)
            preds.loc[:, target_col] += pred
        
        elapsed_time = time.time() - start
        print ("time:{0}".format(elapsed_time) + "[sec]")
    
    preds.loc[:,target_col] /= n_fold * n_seed
    
    print('-------------------------')
    print('finish train and predict!')
    print('-------------------------')
    elapsed_time = time.time() - start_run
    print ('time: {0} '.format(elapsed_time) + '[sec]')
    print('loss:', end=' ')
    med_loss = (max(val_losses) + min(val_losses)) / 2
    pm_loss = max(val_losses) - med_loss
    print('{:.6f} Â± {:.6f} (mean: {:.6f})'.format(med_loss, pm_loss, mean(val_losses)))
    return preds

n_seed = 3
n_fold = 5
output_bias = -np.log(train_y.mean(axis=0))
verbose = 0
debug = False
epoch = 100
activation = 'swish'
plot = False
preds = run(train_x, train_y, test_x, n_seed, n_fold, epoch, debug, verbose, output_bias, activation, plot)

preds.iloc[:,1:] = np.clip(preds.iloc[:,1:].values, p_min, p_max)
preds.loc[ctl_test, target_col] = 0.0

KerasSubmission = preds

"""Combine Them"""

testf = test3[target_cols]*0.9+ test1[target_cols]*0.1
combineresults = pd.concat((test1.iloc[:, 0], testf), axis=1)

sub = sample_submission.drop(columns=target_cols).merge(combineresults[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)
results = (sub.iloc[:, 1:])*(0.50) + (TabNetResult.iloc[:, 1:])*(0.25) + (KerasSubmission.iloc[:, 1:])*(0.25)
combineresults = pd.concat((TabNetResult.iloc[:, 0], results), axis=1)

combineresults.to_csv('/content/drive/My Drive/Submission4ModelsSimpleKeras_75Percent.csv', index=False)

t1 = time.time()
total = t1-t0

print(total)